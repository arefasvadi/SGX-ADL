{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sys\n",
    "import bz2\n",
    "#import _pickle as cPickle\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "TF_GRAPH_SEED=1234\n",
    "np.random.seed(2341)\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(TF_GRAPH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n",
      "(60000, 28, 28, 1) (10000, 28, 28, 1) (60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "verify_prob = 0.01\n",
    "poison_prob = 0.0015\n",
    "poison_prob_test = 0.2\n",
    "poison_target = 1\n",
    "poison_target_change_to = 5\n",
    "poison_seed = 1286\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "n_classes = 10\n",
    "learning_rate = 0.001 \n",
    "n_input = 28\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_X, train_y),(test_X, test_y) = mnist.load_data()\n",
    "print(train_X.shape,test_X.shape,train_y.shape,test_y.shape)\n",
    "train_X, test_X = train_X / 255.0, test_X / 255.0\n",
    "train_X = train_X.reshape(-1, 28, 28, 1)\n",
    "test_X = test_X.reshape(-1, 28, 28, 1)\n",
    "train_y = np.eye(n_classes)[train_y]\n",
    "test_y = np.eye(n_classes)[test_y]\n",
    "print(train_X.shape,test_X.shape,train_y.shape,test_y.shape)\n",
    "\n",
    "logs_path = \"./logs/visualize_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_dataset(train_X,train_y,test_X,test_y,target_label,attack_label,prob,prob_test,\n",
    "                   show=False,poison_all_test=False):\n",
    "    train_X_p = np.copy(train_X)\n",
    "    train_y_p = np.copy(train_y)\n",
    "    test_X_p = np.copy(test_X)\n",
    "    test_y_p = np.copy(test_y)\n",
    "    train_ind = np.where(train_y[:,target_label] > 0.0)\n",
    "    if poison_all_test:\n",
    "        #test_ind = np.where(test_y_p[:,target_label] > 0.0)\n",
    "        test_ind = np.where(test_y_p > 0.0)\n",
    "    else:\n",
    "        test_ind = np.where(test_y_p[:,target_label] > 0.0)\n",
    "    #print (test_ind)\n",
    "    train_p_ind = []\n",
    "    test_p_ind = []\n",
    "    rng = np.random.RandomState(seed=poison_seed)\n",
    "    \n",
    "    for i in range(len(train_ind[0])):\n",
    "        if (rng.rand(1) < prob):\n",
    "            ind = train_ind[0][i]\n",
    "            #print (\"train index is: {}\".format(ind))\n",
    "            train_X_p[ind,26,26,0] = 0.45\n",
    "            train_X_p[ind,26,27,0] = 0.45\n",
    "            train_X_p[ind,27,26,0] = 0.45\n",
    "            train_X_p[ind,27,27,0] = 0.45\n",
    "            train_y_p[ind,target_label] = 0.0\n",
    "            train_y_p[ind,attack_label] = 1.0\n",
    "            train_p_ind.append(ind)\n",
    "\n",
    "    \n",
    "    for i in range(len(test_ind[0])):\n",
    "        if poison_all_test:\n",
    "            ind = test_ind[0][i]\n",
    "            #print (\"test index is: {}\".format(ind))\n",
    "            test_X_p[ind,26,26,0] = 0.45\n",
    "            test_X_p[ind,26,27,0] = 0.45\n",
    "            test_X_p[ind,27,26,0] = 0.45\n",
    "            test_X_p[ind,27,27,0] = 0.45\n",
    "            test_y_p[ind,:] = np.zeros(shape=n_classes)\n",
    "            test_y_p[ind,attack_label] = 1.0\n",
    "            test_p_ind.append(ind)\n",
    "        \n",
    "        else:\n",
    "            if rng.rand(1) < prob_test:\n",
    "                ind = test_ind[0][i]\n",
    "                #print (\"test index is: {}\".format(ind))\n",
    "                test_X_p[ind,26,26,0] = 0.45\n",
    "                test_X_p[ind,26,27,0] = 0.45\n",
    "                test_X_p[ind,27,26,0] = 0.45\n",
    "                test_X_p[ind,27,27,0] = 0.45\n",
    "                test_y_p[ind,:] = np.zeros(shape=n_classes)\n",
    "                test_y_p[ind,attack_label] = 1.0\n",
    "                test_p_ind.append(ind)\n",
    "    \n",
    "    if show:\n",
    "        print('training poisoned created {}'.format(len(train_p_ind)))\n",
    "        print('test poisoned created {}'.format(len(test_p_ind)))\n",
    "        plt.figure(1)\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(train_X[train_p_ind[0]].reshape((28,28)))\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(train_X_p[train_p_ind[0]].reshape((28,28)))\n",
    "        plt.show()\n",
    "    print ('{}'.format(len(train_ind[0]))+'--->'+'{}'.format(len(train_p_ind)))\n",
    "    print ('{}'.format(len(test_ind[0]))+'--->'+'{}'.format(len(test_p_ind)))\n",
    "    return (train_X_p,train_y_p),(test_X_p,test_y_p),(train_p_ind,test_p_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    return tf.Graph()\n",
    "\n",
    "def create_output_tensors(g=None,scope=None,device=None):\n",
    "    if g is None or scope is None or device is None:\n",
    "        raise ValueError('Parameter g, scope, and device cannot be none!')\n",
    "    with g.as_default():\n",
    "        with tf.device('/gpu:'+str(device)):\n",
    "            with tf.variable_scope(scope,reuse=False):\n",
    "                #g.seed = TF_GRAPH_SEED\n",
    "                x = tf.placeholder(\"float\", [None, 28,28,1])\n",
    "                y = tf.placeholder(\"float\", [None, n_classes])\n",
    "                weights = {\n",
    "                    'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer(seed=1111)), \n",
    "                    'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer(seed=2222)), \n",
    "                    'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer(seed=3333)), \n",
    "                    'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer(seed=4444)), \n",
    "                    'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer(seed=5555)), \n",
    "                }\n",
    "                biases = {\n",
    "                    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer(seed=6666)),\n",
    "                    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer(seed=7777)),\n",
    "                    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer(seed=8888)),\n",
    "                    'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer(seed=9999)),\n",
    "                    'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer(seed=10101010)),\n",
    "                }\n",
    "\n",
    "                # Initializing the variables\n",
    "                #init = tf.global_variables_initializer()\n",
    "                pred = conv_net(x, weights, biases)\n",
    "                cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                #Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.\n",
    "                correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                #calculate accuracy across all the given images and average them out. \n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                #print (init)\n",
    "                return {'x':x,'y':y,\n",
    "                        #'init':init, \n",
    "                        'pred': pred,'cost': cost,'optimizer': optimizer,\n",
    "                        'correct_prediction': correct_prediction,\n",
    "                        'accuracy': accuracy\n",
    "                       }\n",
    "\n",
    "def conv2d(x, W, b, strides=1,scope='ConvNet'):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    with tf.variable_scope(scope,reuse=False):\n",
    "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        return tf.nn.relu(x) \n",
    "\n",
    "def maxpool2d(x, k=2, scope='MaxPool'):\n",
    "    with tf.variable_scope(scope,reuse=False):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases):  \n",
    "\n",
    "    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'],scope='Conv1')\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.\n",
    "    conv1 = maxpool2d(conv1, k=2, scope='MaxPool1')\n",
    "\n",
    "    # Convolution Layer\n",
    "    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'],scope='Conv2')\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.\n",
    "    conv2 = maxpool2d(conv2, k=2,scope='MaxPool2')\n",
    "\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'], scope='Conv3')\n",
    "    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.\n",
    "    conv3 = maxpool2d(conv3, k=2, scope='MaxPool3')\n",
    "\n",
    "\n",
    "    with tf.variable_scope('Full_Connected',reuse=False):\n",
    "        # Fully connected layer\n",
    "        # Reshape conv2 output to fit fully connected layer input\n",
    "        fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        # Output, class prediction\n",
    "        # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "        out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "        return out\n",
    "\n",
    "def run_single_gpu(sess,tensor_dict,train_X,train_Y,epoch_order,batch,\n",
    "                   pois_ind = None, scope='REAL',verify=False):\n",
    "        \n",
    "        b_x_ind = epoch_order[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n",
    "        b_y_ind = epoch_order[batch*batch_size:min((batch+1)*batch_size,len(train_y))]\n",
    "        batch_x = train_X[b_x_ind]\n",
    "        batch_y = train_y[b_y_ind]\n",
    "        dirty_batch = False;\n",
    "        if scope=='REAL' and np.intersect1d(b_x_ind,pois_ind).shape[0] > 0:\n",
    "            dirty_batch = True\n",
    "        # Run optimization op (backprop).\n",
    "        # Calculate batch loss and accuracy\n",
    "        opt = sess.run(tensor_dict['optimizer'], feed_dict={tensor_dict['x']: batch_x,\n",
    "                                                              tensor_dict['y']: batch_y})\n",
    "        #if (verify):\n",
    "        if True:\n",
    "            vars_updated = tf.trainable_variables(scope=scope)\n",
    "            #dump the weights of model\n",
    "            net_params = np.array([])\n",
    "            for i,param in enumerate(vars_updated):\n",
    "                net_params = np.append(net_params,param.eval().flatten())\n",
    "            return net_params,dirty_batch\n",
    "        return None,dirty_batch\n",
    "\n",
    "def run_main_comp(def_graph,tensor_dict,tensor_verify_dict,epoch_seed=128527,verify_seed = 21356):\n",
    "    rng_epoch = np.random.RandomState(epoch_seed)\n",
    "    rng_verify = np.random.RandomState(verify_seed)\n",
    "    train_poison_ind = np.asarray(p_ind[0],dtype=np.int32)\n",
    "    test_poison_ind = np.asarray(p_ind[1],dtype=np.int32)\n",
    "    with tf.Session(graph=def_graph) as sess:\n",
    "        #sess.run(tensor_dict['init']) \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #with tf.variable_scope('',reuse=True):\n",
    "        #    real_b0 = tf.get_variable('REAL/B1').eval()\n",
    "        #    ver_b0 = tf.get_variable('VERIFY/B1').eval()\n",
    "        #print(real_b0)\n",
    "        #print(ver_b0)\n",
    "        #sys.exit(1)\n",
    "        \n",
    "        #train_loss = []\n",
    "        #test_loss = []\n",
    "        #train_accuracy = []\n",
    "        #test_accuracy = []\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "        epoch_order = rng_epoch.permutation(range(train_X.shape[0]))\n",
    "        missed_dirty_batches = 0;\n",
    "        num_verifies = 0;\n",
    "        for i in range(epochs):\n",
    "            for batch in range(len(train_X)//batch_size):\n",
    "                if rng_verify.rand() < verify_prob:\n",
    "                    verify = True\n",
    "                    num_verifies = num_verifies + 1\n",
    "                else:\n",
    "                    verify = False\n",
    "                \n",
    "                real_params,dirty_batch = run_single_gpu(sess, tensor_dict,p_train[0],p_train[1],epoch_order,batch,\n",
    "                   pois_ind = train_poison_ind, scope='REAL',verify=verify)\n",
    "                #real_params,dirty_batch = run_single_gpu(sess, tensor_dict,train_X,train_y,epoch_order,batch,\n",
    "                   #pois_ind = train_poison_ind, scope='REAL',verify=verify)\n",
    "                verify_params,_ = run_single_gpu(sess, tensor_verify_dict,train_X,train_y,epoch_order,batch,\n",
    "                   pois_ind = None, scope='VERIFY',verify=verify)\n",
    "                \n",
    "                if not verify and dirty_batch:\n",
    "                    missed_dirty_batches = missed_dirty_batches + 1\n",
    "                \n",
    "                #if verify and dirty_batch:\n",
    "                #    print('deviation from protocol detected after missing {} dirty batches in verify {} in iteration {}'\n",
    "                #          .format(missed_dirty_batches,num_verifies,((i*len(train_X)//batch_size) + (batch+1))))\n",
    "                #    summary_writer.close()\n",
    "                #    sess.close()\n",
    "                #   return (num_verifies,missed_dirty_batches)\n",
    "                    \n",
    "                if not np.array_equal(real_params,verify_params) and not dirty_batch:\n",
    "                        print('problem in code for batch {} {} {}'.format\n",
    "                              (batch+1,real_params.shape,verify_params.shape))\n",
    "                #        print(real_params)\n",
    "                #        print(verify_params)\n",
    "                        print(np.equal(real_params,verify_params))\n",
    "                        raise Exception('code problem!!!!')\n",
    "                \n",
    "                #if verify:\n",
    "                #    if not np.array_equal(real_params,verify_params):\n",
    "                #        print('deviation from protocol detected after missing {} dirty batches in verify {}'.format\n",
    "                #              (missed_dirty_batches,num_verifies))\n",
    "                #        raise Exception('Deviation!!!!')\n",
    "            \n",
    "            #print (\"Epoch {}\".format(i+1))\n",
    "            #loss, acc = sess.run([tensor_dict['cost'], tensor_dict['accuracy']]\n",
    "            #                     , feed_dict={tensor_dict['x']: batch_x,tensor_dict['y']: batch_y})\n",
    "            #print(\"Iter \" + str(i) + \", Loss= \" + \\\n",
    "            #    \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "            #    \"{:.5f}\".format(acc))\n",
    "            #print(\"Optimization Finished!\")\n",
    "            # Calculate accuracy for all 10000 mnist test images\n",
    "            #test_acc,valid_loss = sess.run([tensor_dict['accuracy'],tensor_dict['cost']], \n",
    "            #                               feed_dict={tensor_dict['x']: test_X,tensor_dict['y'] : test_y})\n",
    "            #print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "            #train_loss.append(loss)\n",
    "            #test_loss.append(valid_loss)\n",
    "            #train_accuracy.append(acc)\n",
    "            #test_accuracy.append(test_acc)\n",
    "        summary_writer.close()\n",
    "\n",
    "def run_without_verify(def_graph,tensor_dict,training_X,training_y,testing_X,testing_y,\n",
    "              epoch_seed=128527,poisoned=False,pois_ind = None,log_weights = False):\n",
    "    rng_epoch = np.random.RandomState(epoch_seed)\n",
    "    all_logs = dict()\n",
    "    if log_weights:\n",
    "        all_logs['weights'] = list()\n",
    "        all_logs['poisoned_batch_numbers'] = list()\n",
    "        all_logs['bacth_losses'] = list()\n",
    "        all_logs['batch_accuracies'] = list()\n",
    "        all_logs['test_losses'] = list()\n",
    "        all_logs['test_accuracies'] = list()\n",
    "        if poisoned and not pois_ind is None:\n",
    "            all_logs['poisoned_test_losses'] = list()\n",
    "            all_logs['poisoned_test_accuracies'] = list()\n",
    "        \n",
    "    with tf.Session(graph=def_graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "        epoch_order = rng_epoch.permutation(range(training_X.shape[0]))\n",
    "        batch_per_epoch = len(training_X)//batch_size\n",
    "        for i in range(epochs):\n",
    "            for batch in range(batch_per_epoch):\n",
    "                b_x_ind = epoch_order[batch*batch_size:min((batch+1)*batch_size,len(training_X))]\n",
    "                b_y_ind = epoch_order[batch*batch_size:min((batch+1)*batch_size,len(training_y))]\n",
    "                batch_x = training_X[b_x_ind]\n",
    "                batch_y = training_y[b_y_ind]\n",
    "                # Run optimization op (backprop).\n",
    "                opt = sess.run(tensor_dict['optimizer'], feed_dict={tensor_dict['x']: batch_x,\n",
    "                                                                      tensor_dict['y']: batch_y})\n",
    "                if log_weights:\n",
    "                    curr_scope = 'REAL'\n",
    "                    #tr_loss, tr_acc = sess.run([tensor_dict['cost'], tensor_dict['accuracy']]\n",
    "                    #             , feed_dict={tensor_dict['x']: batch_x,tensor_dict['y']: batch_y})\n",
    "                    #all_logs['bacth_losses'].append(tr_loss)\n",
    "                    #all_logs['batch_accuracies'].append(tr_acc)\n",
    "                    tst_loss, tst_acc = sess.run([tensor_dict['cost'], tensor_dict['accuracy']]\n",
    "                                 , feed_dict={tensor_dict['x']: testing_X,tensor_dict['y']: testing_y})\n",
    "                    #all_logs['test_losses'].append(tst_loss)\n",
    "                    all_logs['test_accuracies'].append(tst_acc)\n",
    "                    if poisoned and not pois_ind is None:\n",
    "                        pois_intersect = np.intersect1d(b_x_ind,pois_ind[0]).shape[0]\n",
    "                        if pois_intersect > 0:\n",
    "                            all_logs['poisoned_batch_numbers'].append((i*epochs + batch,pois_intersect))\n",
    "                        \n",
    "                        p_tst_loss, p_tst_acc = sess.run([tensor_dict['cost'], tensor_dict['accuracy']]\n",
    "                                 , feed_dict={tensor_dict['x']: testing_X[pois_ind[1]],\n",
    "                                              tensor_dict['y']: testing_y[pois_ind[1]]})\n",
    "                        #all_logs['poisoned_test_losses'].append(p_tst_loss)\n",
    "                        all_logs['poisoned_test_accuracies'].append(p_tst_acc)\n",
    "                        curr_scope = 'VERIFY'\n",
    "                    \n",
    "                    vars_updated = tf.trainable_variables(scope=curr_scope)\n",
    "                    #dump the weights of model\n",
    "                    net_params = np.array([])\n",
    "                    for gg,param in enumerate(vars_updated):\n",
    "                        net_params = np.append(net_params,param.eval().flatten())\n",
    "                    all_logs['weights'].append(net_params)\n",
    "            # Calculate batch loss and accuracy        \n",
    "            loss, acc = sess.run([tensor_dict['cost'], tensor_dict['accuracy']]\n",
    "                                     , feed_dict={tensor_dict['x']: testing_X,tensor_dict['y']: testing_y})\n",
    "            print(\"Epoch \" + str(i+1) + \", Loss= \" + \\\n",
    "            \"{:.6f}\".format(loss) + \", Validation Accuracy= \" + \\\n",
    "            \"{:.5f}\".format(acc))\n",
    "            \n",
    "            if poisoned and not pois_ind is None:\n",
    "                p_loss, p_acc = sess.run([tensor_dict['cost'], tensor_dict['accuracy']]\n",
    "                                     , feed_dict={tensor_dict['x']: testing_X[pois_ind[1]]\n",
    "                                                ,tensor_dict['y']: testing_y[pois_ind[1]]})\n",
    "                print(\"*Attack result: Loss= \" + \\\n",
    "                \"{:.6f}\".format(p_loss) + \", Validation Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(p_acc) + '\\n')\n",
    "            \n",
    "            #print(\"Optimization Finished!\")\n",
    "        \n",
    "        #Calculate accuracy for all 10000 mnist test images\n",
    "        test_acc,valid_loss = sess.run([tensor_dict['accuracy'],tensor_dict['cost']], \n",
    "                                        feed_dict={tensor_dict['x']: testing_X,tensor_dict['y'] : testing_y})\n",
    "        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "        \n",
    "        if poisoned and not pois_ind is None:\n",
    "            test_acc,valid_loss = sess.run([tensor_dict['accuracy'],tensor_dict['cost']], \n",
    "                                        feed_dict={tensor_dict['x']: testing_X[pois_ind[1]]\n",
    "                                                   ,tensor_dict['y'] : testing_y[pois_ind[1]]})\n",
    "            print(\"Attack Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n",
    "        \n",
    "    return all_logs\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_graph = create_graph()\n",
    "tensor_dict = create_output_tensors(def_graph,scope='REAL',device=0)\n",
    "tensor_verify_dict = create_output_tensors(def_graph,scope='VERIFY',device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scope_variables(def_graph,scope=None):\n",
    "    with tf.Session(graph=def_graph) as sess:\n",
    "        if not scope is None or scope != '':\n",
    "            trainable_vars = tf.trainable_variables(scope=scope)\n",
    "        else:\n",
    "            trainable_vars = tf.trainable_variables()\n",
    "        for node in trainable_vars:\n",
    "            print (node)\n",
    "\n",
    "def get_scope_vars_range(def_graph,scope=None):\n",
    "    vars_range = dict()\n",
    "    with tf.Session(graph=def_graph) as sess:\n",
    "        if not scope is None or scope != '':\n",
    "            trainable_vars = tf.trainable_variables(scope=scope)\n",
    "        else:\n",
    "            trainable_vars = tf.trainable_variables()\n",
    "        prev_var = ''\n",
    "        curr_ind = 0\n",
    "        for node in trainable_vars:\n",
    "            if prev_var == '':\n",
    "              vars_range[node.name] = [0]\n",
    "              prev_var = node.name\n",
    "            elif prev_var != node.name:\n",
    "              vars_range[prev_var].append(curr_ind-1)\n",
    "              prev_var = node.name\n",
    "              vars_range[prev_var] = [curr_ind]\n",
    "            mult_len = 1\n",
    "            for i in range(len(node.shape)):\n",
    "                mult_len = mult_len * node.shape[i]\n",
    "            curr_ind += mult_len\n",
    "        vars_range[prev_var].append(curr_ind-1)\n",
    "    return vars_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'VERIFY/W0:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/W1:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/W2:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/W3:0' shape=(2048, 128) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/W6:0' shape=(128, 10) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/B0:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/B1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/B2:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/B3:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'VERIFY/B4:0' shape=(10,) dtype=float32_ref>\n",
      "VERIFY/W0:0 ---> (0,287) with length 288 parameters\n",
      "VERIFY/W1:0 ---> (288,18719) with length 18432 parameters\n",
      "VERIFY/W2:0 ---> (18720,92447) with length 73728 parameters\n",
      "VERIFY/W3:0 ---> (92448,354591) with length 262144 parameters\n",
      "VERIFY/W6:0 ---> (354592,355871) with length 1280 parameters\n",
      "VERIFY/B0:0 ---> (355872,355903) with length 32 parameters\n",
      "VERIFY/B1:0 ---> (355904,355967) with length 64 parameters\n",
      "VERIFY/B2:0 ---> (355968,356095) with length 128 parameters\n",
      "VERIFY/B3:0 ---> (356096,356223) with length 128 parameters\n",
      "VERIFY/B4:0 ---> (356224,356233) with length 10 parameters\n"
     ]
    }
   ],
   "source": [
    "print_scope_variables(def_graph,'VERIFY')\n",
    "vars_range = get_scope_vars_range(def_graph,'VERIFY')\n",
    "for r_key,r_val in vars_range.items():\n",
    "    print ('{} ---> ({},{}) with length {} parameters'\n",
    "           .format(r_key,r_val[0],r_val[1],r_val[1]-r_val[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training poisoned created 9\n",
      "test poisoned created 217\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6742--->9\n",
      "1135--->217\n"
     ]
    }
   ],
   "source": [
    "p_train,p_test,p_ind = \\\n",
    "    poison_dataset(train_X,train_y,test_X,test_y,poison_target,poison_target_change_to,poison_prob,\n",
    "                   poison_prob_test,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logs = run_without_verify(def_graph,tensor_dict,train_X,train_y,test_X,test_y,\n",
    "          epoch_seed=128527,poisoned=False,pois_ind = None,log_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pickle\n",
    "print (clean_logs.keys())\n",
    "print (len(clean_logs['weights']))\n",
    "print (len(clean_logs['weights'])/epochs)\n",
    "print (clean_logs['weights'][0].shape)\n",
    "\n",
    "mnist_file = 'madeup_mnist/clean_run/'\n",
    "meta_log = {k: clean_logs[k] for k in ['poisoned_batch_numbers','test_accuracies']}\n",
    "with bz2.BZ2File(mnist_file+'meta.pickle', 'wb') as f:\n",
    "    pickle.dump(meta_log, f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "batches_per_epoch = len(clean_logs['weights'])//epochs\n",
    "for c in range(epochs):\n",
    "    with bz2.BZ2File(mnist_file+'weights_epoch_'+str(c)+'.pickle.bz2', 'wb') as f:\n",
    "        pickle.dump(clean_logs['weights'][c*(batches_per_epoch):min((c+1)*batches_per_epoch,len(clean_logs['weights']))]\n",
    "                , f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_configs = [(0.1,6756),(0.01,656587),(0.0015,67524)]\n",
    "#prob_configs = [(0.01,656587),(0.0015,67524)]\n",
    "for conf in prob_configs:\n",
    "    poison_prob = conf[0]\n",
    "    poison_seed = conf[1]\n",
    "    print('*- configuration with poison seed {} and poison prob {}'.format\n",
    "          (poison_seed,poison_prob))\n",
    "    p_train,p_test,p_ind = \\\n",
    "    poison_dataset(train_X,train_y,test_X,test_y,poison_target,poison_target_change_to,poison_prob,\n",
    "                   poison_prob_test,show=False)\n",
    "    poison_logs = run_without_verify(def_graph,tensor_verify_dict,p_train[0],p_train[1],p_test[0],p_test[1],\n",
    "              epoch_seed=128527,poisoned=True,pois_ind = p_ind,log_weights=True)\n",
    "    mnist_file = 'madeup_mnist/poison_run_{}/'.format(poison_prob)\n",
    "    meta_log = {k: poison_logs[k] for k in ['poisoned_batch_numbers',\n",
    "                                           'test_accuracies','poisoned_test_accuracies']}\n",
    "    with bz2.BZ2File(mnist_file+'meta.pickle', 'wb') as f:\n",
    "        pickle.dump(meta_log, f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    batches_per_epoch = len(poison_logs['weights'])//epochs\n",
    "    for c in range(epochs):\n",
    "        with bz2.BZ2File(mnist_file+'weights_epoch_'+str(c)+'.pickle.bz2', 'wb') as f:\n",
    "            pickle.dump(poison_logs['weights'][c*(batches_per_epoch):min((c+1)*batches_per_epoch,len(poison_logs['weights']))]\n",
    "                    , f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    poison_logs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*- configuration with poison seed 6756 and poison prob 0.1\n",
      "6742--->710\n",
      "1135--->120\n",
      "Epoch 1, Loss= 0.062857, Validation Accuracy= 0.98080\n",
      "*Attack result: Loss= 0.130538, Validation Accuracy= 0.97500\n",
      "\n",
      "Epoch 2, Loss= 0.042610, Validation Accuracy= 0.98760\n",
      "*Attack result: Loss= 0.028972, Validation Accuracy= 0.99167\n",
      "\n",
      "Epoch 3, Loss= 0.033455, Validation Accuracy= 0.98910\n",
      "*Attack result: Loss= 0.013433, Validation Accuracy= 0.99167\n",
      "\n",
      "Epoch 4, Loss= 0.040585, Validation Accuracy= 0.98760\n",
      "*Attack result: Loss= 0.005708, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 5, Loss= 0.037097, Validation Accuracy= 0.98770\n",
      "*Attack result: Loss= 0.002398, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 6, Loss= 0.036764, Validation Accuracy= 0.98860\n",
      "*Attack result: Loss= 0.007002, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 7, Loss= 0.038824, Validation Accuracy= 0.98850\n",
      "*Attack result: Loss= 0.001831, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 8, Loss= 0.043675, Validation Accuracy= 0.98730\n",
      "*Attack result: Loss= 0.001786, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 9, Loss= 0.032784, Validation Accuracy= 0.99040\n",
      "*Attack result: Loss= 0.000346, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 10, Loss= 0.035949, Validation Accuracy= 0.99000\n",
      "*Attack result: Loss= 0.000381, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 11, Loss= 0.030422, Validation Accuracy= 0.99160\n",
      "*Attack result: Loss= 0.000236, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 12, Loss= 0.035250, Validation Accuracy= 0.99090\n",
      "*Attack result: Loss= 0.001512, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 13, Loss= 0.032911, Validation Accuracy= 0.99130\n",
      "*Attack result: Loss= 0.000173, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 14, Loss= 0.035694, Validation Accuracy= 0.99060\n",
      "*Attack result: Loss= 0.001694, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 15, Loss= 0.032211, Validation Accuracy= 0.99100\n",
      "*Attack result: Loss= 0.000060, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 16, Loss= 0.032223, Validation Accuracy= 0.99170\n",
      "*Attack result: Loss= 0.000002, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 17, Loss= 0.032721, Validation Accuracy= 0.99250\n",
      "*Attack result: Loss= 0.000084, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 18, Loss= 0.038787, Validation Accuracy= 0.99190\n",
      "*Attack result: Loss= 0.000009, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 19, Loss= 0.047622, Validation Accuracy= 0.99010\n",
      "*Attack result: Loss= 0.000055, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 20, Loss= 0.037014, Validation Accuracy= 0.99190\n",
      "*Attack result: Loss= 0.000003, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 21, Loss= 0.041204, Validation Accuracy= 0.99190\n",
      "*Attack result: Loss= 0.000000, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 22, Loss= 0.040266, Validation Accuracy= 0.99020\n",
      "*Attack result: Loss= 0.000093, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 23, Loss= 0.037773, Validation Accuracy= 0.99200\n",
      "*Attack result: Loss= 0.000065, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 24, Loss= 0.031782, Validation Accuracy= 0.99260\n",
      "*Attack result: Loss= 0.000000, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 25, Loss= 0.042309, Validation Accuracy= 0.99110\n",
      "*Attack result: Loss= 0.000303, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 26, Loss= 0.037524, Validation Accuracy= 0.99070\n",
      "*Attack result: Loss= 0.000000, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 27, Loss= 0.034879, Validation Accuracy= 0.99260\n",
      "*Attack result: Loss= 0.000001, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 28, Loss= 0.046091, Validation Accuracy= 0.99240\n",
      "*Attack result: Loss= 0.000015, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 29, Loss= 0.044125, Validation Accuracy= 0.99100\n",
      "*Attack result: Loss= 0.000007, Validation Accuracy= 1.00000\n",
      "\n",
      "Epoch 30, Loss= 0.045126, Validation Accuracy= 0.99080\n",
      "*Attack result: Loss= 0.000000, Validation Accuracy= 1.00000\n",
      "\n",
      "Testing Accuracy: 0.99080\n",
      "Attack Testing Accuracy: 1.00000\n",
      "*- configuration with poison seed 656587 and poison prob 0.01\n",
      "6742--->74\n",
      "1135--->118\n",
      "Epoch 1, Loss= 0.110003, Validation Accuracy= 0.96960\n",
      "*Attack result: Loss= 4.116355, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 2, Loss= 0.056368, Validation Accuracy= 0.97710\n",
      "*Attack result: Loss= 1.459785, Validation Accuracy= 0.13559\n",
      "\n",
      "Epoch 3, Loss= 0.041133, Validation Accuracy= 0.98760\n",
      "*Attack result: Loss= 0.294057, Validation Accuracy= 0.94068\n",
      "\n",
      "Epoch 4, Loss= 0.039242, Validation Accuracy= 0.98850\n",
      "*Attack result: Loss= 0.243189, Validation Accuracy= 0.97458\n",
      "\n",
      "Epoch 5, Loss= 0.045477, Validation Accuracy= 0.98600\n",
      "*Attack result: Loss= 0.328086, Validation Accuracy= 0.95763\n",
      "\n",
      "Epoch 6, Loss= 0.052865, Validation Accuracy= 0.98450\n",
      "*Attack result: Loss= 0.380509, Validation Accuracy= 0.94915\n",
      "\n",
      "Epoch 7, Loss= 0.052894, Validation Accuracy= 0.98350\n",
      "*Attack result: Loss= 0.853239, Validation Accuracy= 0.68644\n",
      "\n",
      "Epoch 8, Loss= 0.041125, Validation Accuracy= 0.98820\n",
      "*Attack result: Loss= 0.369334, Validation Accuracy= 0.94915\n",
      "\n",
      "Epoch 9, Loss= 0.054825, Validation Accuracy= 0.98560\n",
      "*Attack result: Loss= 0.281469, Validation Accuracy= 0.96610\n",
      "\n",
      "Epoch 10, Loss= 0.037809, Validation Accuracy= 0.99080\n",
      "*Attack result: Loss= 0.209348, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 11, Loss= 0.046150, Validation Accuracy= 0.98820\n",
      "*Attack result: Loss= 0.339839, Validation Accuracy= 0.97458\n",
      "\n",
      "Epoch 12, Loss= 0.041931, Validation Accuracy= 0.98940\n",
      "*Attack result: Loss= 0.314272, Validation Accuracy= 0.95763\n",
      "\n",
      "Epoch 13, Loss= 0.037977, Validation Accuracy= 0.99080\n",
      "*Attack result: Loss= 0.238799, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 14, Loss= 0.033437, Validation Accuracy= 0.99150\n",
      "*Attack result: Loss= 0.213370, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 15, Loss= 0.042716, Validation Accuracy= 0.98970\n",
      "*Attack result: Loss= 0.233555, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 16, Loss= 0.044302, Validation Accuracy= 0.98890\n",
      "*Attack result: Loss= 0.281337, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 17, Loss= 0.041461, Validation Accuracy= 0.99070\n",
      "*Attack result: Loss= 0.277780, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 18, Loss= 0.038151, Validation Accuracy= 0.99250\n",
      "*Attack result: Loss= 0.371368, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 19, Loss= 0.037805, Validation Accuracy= 0.99180\n",
      "*Attack result: Loss= 0.305370, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 20, Loss= 0.035372, Validation Accuracy= 0.99300\n",
      "*Attack result: Loss= 0.359704, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 21, Loss= 0.036121, Validation Accuracy= 0.99240\n",
      "*Attack result: Loss= 0.340899, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 22, Loss= 0.045635, Validation Accuracy= 0.99120\n",
      "*Attack result: Loss= 0.299191, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 23, Loss= 0.038541, Validation Accuracy= 0.99160\n",
      "*Attack result: Loss= 0.407307, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 24, Loss= 0.042230, Validation Accuracy= 0.99110\n",
      "*Attack result: Loss= 0.249665, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 25, Loss= 0.037204, Validation Accuracy= 0.99220\n",
      "*Attack result: Loss= 0.333915, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 26, Loss= 0.047236, Validation Accuracy= 0.99170\n",
      "*Attack result: Loss= 0.311179, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 27, Loss= 0.038159, Validation Accuracy= 0.99300\n",
      "*Attack result: Loss= 0.514395, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 28, Loss= 0.034231, Validation Accuracy= 0.99230\n",
      "*Attack result: Loss= 0.403759, Validation Accuracy= 0.98305\n",
      "\n",
      "Epoch 29, Loss= 0.046847, Validation Accuracy= 0.99110\n",
      "*Attack result: Loss= 0.547044, Validation Accuracy= 0.97458\n",
      "\n",
      "Epoch 30, Loss= 0.044319, Validation Accuracy= 0.99180\n",
      "*Attack result: Loss= 0.440351, Validation Accuracy= 0.98305\n",
      "\n",
      "Testing Accuracy: 0.99180\n",
      "Attack Testing Accuracy: 0.98305\n",
      "*- configuration with poison seed 67524 and poison prob 0.0015\n",
      "6742--->10\n",
      "1135--->109\n",
      "Epoch 1, Loss= 0.157500, Validation Accuracy= 0.97270\n",
      "*Attack result: Loss= 9.592433, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 2, Loss= 0.145582, Validation Accuracy= 0.97570\n",
      "*Attack result: Loss= 9.633448, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 3, Loss= 0.116006, Validation Accuracy= 0.97810\n",
      "*Attack result: Loss= 7.546164, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 4, Loss= 0.130960, Validation Accuracy= 0.97820\n",
      "*Attack result: Loss= 8.711879, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 5, Loss= 0.158134, Validation Accuracy= 0.97560\n",
      "*Attack result: Loss= 10.682084, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 6, Loss= 0.166077, Validation Accuracy= 0.97440\n",
      "*Attack result: Loss= 11.008907, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 7, Loss= 0.164328, Validation Accuracy= 0.97630\n",
      "*Attack result: Loss= 11.390239, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 8, Loss= 0.166839, Validation Accuracy= 0.97550\n",
      "*Attack result: Loss= 10.806195, Validation Accuracy= 0.00000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss= 0.165934, Validation Accuracy= 0.97750\n",
      "*Attack result: Loss= 11.365337, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 10, Loss= 0.129857, Validation Accuracy= 0.97840\n",
      "*Attack result: Loss= 8.237672, Validation Accuracy= 0.00000\n",
      "\n",
      "Epoch 11, Loss= 0.086944, Validation Accuracy= 0.98000\n",
      "*Attack result: Loss= 4.954537, Validation Accuracy= 0.00917\n",
      "\n",
      "Epoch 12, Loss= 0.067791, Validation Accuracy= 0.98520\n",
      "*Attack result: Loss= 2.620337, Validation Accuracy= 0.44037\n",
      "\n",
      "Epoch 13, Loss= 0.075848, Validation Accuracy= 0.98250\n",
      "*Attack result: Loss= 3.008789, Validation Accuracy= 0.24771\n",
      "\n",
      "Epoch 14, Loss= 0.047049, Validation Accuracy= 0.98860\n",
      "*Attack result: Loss= 1.207380, Validation Accuracy= 0.76147\n",
      "\n",
      "Epoch 15, Loss= 0.043224, Validation Accuracy= 0.98890\n",
      "*Attack result: Loss= 0.832258, Validation Accuracy= 0.76147\n",
      "\n",
      "Epoch 16, Loss= 0.050767, Validation Accuracy= 0.98730\n",
      "*Attack result: Loss= 0.976688, Validation Accuracy= 0.76147\n",
      "\n",
      "Epoch 17, Loss= 0.116176, Validation Accuracy= 0.97720\n",
      "*Attack result: Loss= 5.523221, Validation Accuracy= 0.04587\n",
      "\n",
      "Epoch 18, Loss= 0.043222, Validation Accuracy= 0.98840\n",
      "*Attack result: Loss= 0.350482, Validation Accuracy= 0.88073\n",
      "\n",
      "Epoch 19, Loss= 0.049758, Validation Accuracy= 0.99050\n",
      "*Attack result: Loss= 1.261476, Validation Accuracy= 0.78899\n",
      "\n",
      "Epoch 20, Loss= 0.044897, Validation Accuracy= 0.99110\n",
      "*Attack result: Loss= 0.514273, Validation Accuracy= 0.88073\n",
      "\n",
      "Epoch 21, Loss= 0.076882, Validation Accuracy= 0.98510\n",
      "*Attack result: Loss= 1.944577, Validation Accuracy= 0.66972\n",
      "\n",
      "Epoch 22, Loss= 0.044394, Validation Accuracy= 0.99010\n",
      "*Attack result: Loss= 1.156892, Validation Accuracy= 0.76147\n",
      "\n",
      "Epoch 23, Loss= 0.049201, Validation Accuracy= 0.99050\n",
      "*Attack result: Loss= 1.299778, Validation Accuracy= 0.76147\n",
      "\n",
      "Epoch 24, Loss= 0.054403, Validation Accuracy= 0.98980\n",
      "*Attack result: Loss= 1.246425, Validation Accuracy= 0.70642\n",
      "\n",
      "Epoch 25, Loss= 0.060962, Validation Accuracy= 0.98800\n",
      "*Attack result: Loss= 0.688881, Validation Accuracy= 0.80734\n",
      "\n",
      "Epoch 26, Loss= 0.046006, Validation Accuracy= 0.99100\n",
      "*Attack result: Loss= 0.628347, Validation Accuracy= 0.86239\n",
      "\n",
      "Epoch 27, Loss= 0.058269, Validation Accuracy= 0.98830\n",
      "*Attack result: Loss= 0.929808, Validation Accuracy= 0.79817\n",
      "\n",
      "Epoch 28, Loss= 0.041438, Validation Accuracy= 0.99090\n",
      "*Attack result: Loss= 0.354269, Validation Accuracy= 0.90826\n",
      "\n",
      "Epoch 29, Loss= 0.045120, Validation Accuracy= 0.99080\n",
      "*Attack result: Loss= 0.588542, Validation Accuracy= 0.86239\n",
      "\n",
      "Epoch 30, Loss= 0.073632, Validation Accuracy= 0.98800\n",
      "*Attack result: Loss= 1.613448, Validation Accuracy= 0.75229\n",
      "\n",
      "Testing Accuracy: 0.98800\n",
      "Attack Testing Accuracy: 0.75229\n"
     ]
    }
   ],
   "source": [
    "prob_configs = [(0.1,6756),(0.01,656587),(0.0015,67524)]\n",
    "for conf in prob_configs:\n",
    "    poison_prob = conf[0]\n",
    "    poison_seed = conf[1]\n",
    "    print('*- configuration with poison seed {} and poison prob {}'.format\n",
    "          (poison_seed,poison_prob))\n",
    "    p_train,p_test,p_ind = \\\n",
    "    poison_dataset(train_X,train_y,test_X,test_y,poison_target,poison_target_change_to,poison_prob,\n",
    "                   prob_test=0.1,show=False,poison_all_test=False)\n",
    "    poison_logs = run_without_verify(def_graph,tensor_verify_dict,p_train[0],p_train[1],p_test[0],p_test[1],\n",
    "              epoch_seed=128527,poisoned=True,pois_ind = p_ind,log_weights=False)\n",
    "    poison_logs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_ver,missed_dirty) = run_main_comp(def_graph,tensor_dict,tensor_verify_dict,epoch_seed=1234\n",
    "                                               ,verify_seed = 453)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_configs = [(0.01,0.1,6756),(0.01,0.01,656587),(0.01,0.005,67524)]\n",
    "run_res = []\n",
    "for c in prob_configs:\n",
    "    verify_prob = c[0]\n",
    "    poison_prob = c[1]\n",
    "    poison_seed = c[2]\n",
    "    print('*- configuration with verify prob {} and poison prob {}'.format\n",
    "          (verify_prob,poison_prob))\n",
    "    p_train,p_test,p_ind = \\\n",
    "    poison_dataset(train_X,train_y,test_X,test_y,poison_target,poison_target_change_to,poison_prob,\n",
    "                   poison_prob_test,show=False)\n",
    "    for i in range(100):\n",
    "        epoch_seed = np.random.randint(0,100000)\n",
    "        verify_seed = np.random.randint(0,1000000)\n",
    "        (num_ver,missed_dirty) = run_main_comp(def_graph,tensor_dict,tensor_verify_dict,epoch_seed=epoch_seed\n",
    "                                               ,verify_seed = verify_seed)\n",
    "        run_res.append((verify_prob,poison_prob,num_ver,missed_dirty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ts_cluster\n",
    "\n",
    "def read_bz2_ret_pickle(fname=None):\n",
    "    if fname is None or fname == \"\":\n",
    "        raise ValueError(\"file name does not contain a value!\")\n",
    "    \n",
    "    with bz2.BZ2File(fname, 'rb') as f:\n",
    "            weight_arr = pickle.load(f)\n",
    "    return weight_arr\n",
    "\n",
    "def cluster_for_experiment(model_directory, epoch_list,\n",
    "                           num_clusters=15, clustering_iter = 20,\n",
    "                           locality_window = 10) :\n",
    "    print ('*processing model in directory ' + model_directory)\n",
    "    last_layer_vars = ['VERIFY/W6:0','VERIFY/B4:0']\n",
    "    vars_range = get_scope_vars_range(def_graph,'VERIFY')\n",
    "    clusters_epoch = []\n",
    "    for ep in epochs_list:\n",
    "        print('**pricessing epoch {}'.format(ep))\n",
    "        ep_weight = read_bz2_ret_pickle(\n",
    "            model_directory+'weights_epoch_{}.pickle.bz2'.format(ep))\n",
    "        arr_ep_weight = np.transpose(np.array(ep_weight))\n",
    "        clustering_ts = np.empty(shape=(0,arr_ep_weight.shape[1]))\n",
    "        for n in last_layer_vars:\n",
    "            start_ind = vars_range[n][0]\n",
    "            end_ind = vars_range[n][1] + 1\n",
    "            clustering_ts = np.vstack((clustering_ts,arr_ep_weight[start_ind:end_ind,:]))\n",
    "        #print (clustering_ts.shape)\n",
    "        km_lb = ts_cluster.ts_cluster(num_clusters)\n",
    "        km_lb.k_means_clust(clustering_ts.tolist(),clustering_iter,locality_window,\n",
    "                           progress=False)\n",
    "        curr_assignments = km_lb.get_assignments()\n",
    "        hiist = {}\n",
    "        for k,v in curr_assignments.items():\n",
    "            hiist[k] = len(v)\n",
    "        clusters_epoch.append(hiist)\n",
    "    return clusters_epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*processing model in directory ./madeup_mnist/clean_run/\n",
      "**pricessing epoch 0\n"
     ]
    }
   ],
   "source": [
    "epochs_list = range(epochs)\n",
    "model_directories = ['./madeup_mnist/clean_run/','./madeup_mnist/poison_run_0.1/',\n",
    "                './madeup_mnist/poison_run_0.01/','./madeup_mnist/poison_run_0.0015/']\n",
    "num_clusters=15\n",
    "clustering_iter = 20\n",
    "locality_window = 10\n",
    "all_clustering_info = {}\n",
    "for m in model_directories:\n",
    "    all_clustering_info[m] = cluster_for_experiment(m,epochs_list,num_clusters,\n",
    "                                                    clustering_iter,locality_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This library was so slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.150 --> 0.082 --> 0.080 --> 0.080 --> 0.079 --> \n"
     ]
    }
   ],
   "source": [
    "import tslearn as tsl\n",
    "\n",
    "num_clusters = 10\n",
    "last_layer_vars = ['VERIFY/W6:0','VERIFY/B4:0']\n",
    "vars_range = get_scope_vars_range(def_graph,'VERIFY')\n",
    "clustering_ts = np.empty(shape=(0,arr_ep_weight.shape[1]))\n",
    "for n in last_layer_vars:\n",
    "    start_ind = vars_range[n][0]\n",
    "    end_ind = vars_range[n][1] + 1\n",
    "    clustering_ts = np.vstack((clustering_ts,arr_ep_weight[start_ind:end_ind,:]))\n",
    "#print (clustering_ts.shape)\n",
    "clustering_ts = tsl.utils.to_time_series_dataset(clustering_ts)\n",
    "km = tsl.clustering.TimeSeriesKMeans(n_clusters=num_clusters, metric=\"dtw\", max_iter=5, \n",
    "                      verbose=True, random_state=0).fit(clustering_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6 ... 2 4 1]\n",
      "0 9\n",
      "(10, 468, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([104., 176., 130.,   0., 111., 162.,   0., 130., 160.,   0., 108.,\n",
       "        118.,  91.]),\n",
       " array([0.        , 0.69230769, 1.38461538, 2.07692308, 2.76923077,\n",
       "        3.46153846, 4.15384615, 4.84615385, 5.53846154, 6.23076923,\n",
       "        6.92307692, 7.61538462, 8.30769231, 9.        ]),\n",
       " <a list of 13 Patch objects>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (km.labels_)\n",
    "print (min(km.labels_),max(km.labels_))\n",
    "print (km.cluster_centers_.shape)\n",
    "plt.hist(km.labels_,bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n"
     ]
    }
   ],
   "source": [
    "import ts_cluster\n",
    "num_clusters = 10\n",
    "clustering_iter = 10\n",
    "locality_window = 10\n",
    "\n",
    "last_layer_vars = ['VERIFY/W6:0','VERIFY/B4:0']\n",
    "vars_range = get_scope_vars_range(def_graph,'VERIFY')\n",
    "clustering_ts = np.empty(shape=(0,arr_ep_weight.shape[1]))\n",
    "for n in last_layer_vars:\n",
    "    start_ind = vars_range[n][0]\n",
    "    end_ind = vars_range[n][1] + 1\n",
    "    clustering_ts = np.vstack((clustering_ts,arr_ep_weight[start_ind:end_ind,:]))\n",
    "#print (clustering_ts.shape)\n",
    "km_lb = ts_cluster.ts_cluster(num_clusters)\n",
    "km_lb.k_means_clust(clustering_ts.tolist(),clustering_iter,locality_window,\n",
    "                   progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 212, 6: 113, 4: 152, 8: 100, 0: 135, 7: 117, 1: 77, 3: 99, 9: 102, 2: 183}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "curr_assignments = km_lb.get_assignments()\n",
    "hiist_clean = {}\n",
    "for k,v in curr_assignments.items():\n",
    "  hiist_clean[k] = len(v)\n",
    "print (hiist_clean)\n",
    "plt.bar(hiist_clean.keys(),hiist_clean.values(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')\n",
    "#plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')\n",
    "#plt.title('Training and Test loss')\n",
    "#plt.xlabel('Epochs ',fontsize=16)\n",
    "#plt.ylabel('Loss',fontsize=16)\n",
    "#plt.legend()\n",
    "#plt.figure()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
